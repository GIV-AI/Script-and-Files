Namespace: s-jain-training-10
Pod Name: projectday3triton-0
Status: 2/2
Other Details: Running                    0                100m
Name:             projectday3triton-0
Namespace:        s-jain-training-10
Priority:         0
Service Account:  default-editor
Node:             jain-dgx-server/192.168.1.248
Start Time:       Mon, 21 Apr 2025 10:22:22 +0000
Labels:           app=projectday3triton
                  controller-revision-hash=projectday3triton-d4f8cf6b6
                  notebook-name=projectday3triton
                  security.istio.io/tlsMode=istio
                  service.istio.io/canonical-name=projectday3triton
                  service.istio.io/canonical-revision=latest
                  statefulset=projectday3triton
                  statefulset.kubernetes.io/pod-name=projectday3triton-0
Annotations:      cni.projectcalico.org/containerID: 08ce9bb4a4068ab38bef6ff0c2c9a08c2c2b4c6b90714ad79642ebe04339a23e
                  cni.projectcalico.org/podIP: 10.233.120.13/32
                  cni.projectcalico.org/podIPs: 10.233.120.13/32
                  kubectl.kubernetes.io/default-container: projectday3triton
                  kubectl.kubernetes.io/default-logs-container: projectday3triton
                  prometheus.io/path: /stats/prometheus
                  prometheus.io/port: 15020
                  prometheus.io/scrape: true
                  sidecar.istio.io/status:
                    {"initContainers":["istio-init"],"containers":["istio-proxy"],"volumes":["workload-socket","credential-socket","workload-certs","istio-env...
Status:           Running
IP:               10.233.120.13
IPs:
  IP:           10.233.120.13
Controlled By:  StatefulSet/projectday3triton
Init Containers:
  istio-init:
    Container ID:  docker://169aff8e6c99488a222206dd71614a5913394bcdc501dadd7eb419070b5c0814
    Image:         docker.io/istio/proxyv2:1.16.0
    Image ID:      docker-pullable://istio/proxyv2@sha256:f6f97fa4fb77a3cbe1e3eca0fa46bd462ad6b284c129cf57bf91575c4fb50cf9
    Port:          <none>
    Host Port:     <none>
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      *
      -d
      15090,15021,15020
      --log_output_level=default:info
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Mon, 21 Apr 2025 10:22:23 +0000
      Finished:     Mon, 21 Apr 2025 10:22:23 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     2
      memory:  1Gi
    Requests:
      cpu:        10m
      memory:     40Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-72czk (ro)
Containers:
  projectday3triton:
    Container ID:   docker://aef4364fd69b4b7cd4aca10381e92939f9d154f2d7de27f226516fddda07c87a
    Image:          nvcr.io/nvidia/tritonserver:23.09-py3-sdk_kubeflow
    Image ID:       docker://sha256:b46b7832f8fb950d030b9bcf65efbe3547a038615117c935876f163bdb3a7ba4
    Port:           8888/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Mon, 21 Apr 2025 10:22:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:                     9600m
      memory:                  20615843020800m
      nvidia.com/mig-1g.18gb:  1
    Requests:
      cpu:                     8
      memory:                  16Gi
      nvidia.com/mig-1g.18gb:  1
    Environment:
      NB_PREFIX:  /notebook/s-jain-training-10/projectday3triton
    Mounts:
      /dev/shm from dshm (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-72czk (ro)
  istio-proxy:
    Container ID:  docker://0d3b12bad090eb4a6129f2db842b38e5fd211683e62999cf61fe73546a1a7c99
    Image:         docker.io/istio/proxyv2:1.16.0
    Image ID:      docker-pullable://istio/proxyv2@sha256:f6f97fa4fb77a3cbe1e3eca0fa46bd462ad6b284c129cf57bf91575c4fb50cf9
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --log_output_level=default:info
      --concurrency
      2
    State:          Running
      Started:      Mon, 21 Apr 2025 10:22:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     2
      memory:  1Gi
    Requests:
      cpu:      10m
      memory:   40Mi
    Readiness:  http-get http://:15021/healthz/ready delay=1s timeout=3s period=2s #success=1 #failure=30
    Environment:
      JWT_POLICY:                    third-party-jwt
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      projectday3triton-0 (v1:metadata.name)
      POD_NAMESPACE:                 s-jain-training-10 (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      PROXY_CONFIG:                  {}
                                     
      ISTIO_META_POD_PORTS:          [
                                         {"name":"notebook-port","containerPort":8888,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     projectday3triton
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_META_WORKLOAD_NAME:      projectday3triton
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/s-jain-training-10/statefulsets/projectday3triton
      ISTIO_META_MESH_ID:            cluster.local
      TRUST_DOMAIN:                  cluster.local
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/credential-uds from credential-socket (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-72czk (ro)
      /var/run/secrets/tokens from istio-token (rw)
      /var/run/secrets/workload-spiffe-credentials from workload-certs (rw)
      /var/run/secrets/workload-spiffe-uds from workload-socket (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  workload-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  credential-socket:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  workload-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -> labels
      metadata.annotations -> annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      istio-ca-root-cert
    Optional:  false
  dshm:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  kube-api-access-72czk:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>
